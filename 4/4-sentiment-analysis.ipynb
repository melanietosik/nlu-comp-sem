{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis (Exercise 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Paloma Jeretic\"\n",
    "__version__ = \"DSGA 1012, NYU, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the Stanford Sentiment Treebank. If you don't already have it, download it from here: [the train/dev/test Stanford Sentiment Treebank distribution](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip), unzip it, and put the resulting folder in the same directory as this notebook. (If you want to put it somewhere else, change `sst_home` below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = 'data/trees'\n",
    "\n",
    "def load_sst_data(path):\n",
    "    # Let's do 2-way positive/negative classification instead of 5-way\n",
    "    EASY_LABEL_MAP = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "    \n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = EASY_LABEL_MAP[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "# test_set = load_sst_data(sst_home + '/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using IMDb movie reviews as a test set later on. Download the data <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">here</a>, unzip it, and put the resulting folder in the same directory as this notebook.\n",
    "\n",
    "The following function reformats it in the same form as our SST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_home = 'data/aclImdb/test/'\n",
    "\n",
    "def load_imdb_data(path):\n",
    "    \n",
    "    pos_data, neg_data = [], []\n",
    "    all_files = []\n",
    "    _limit = 250\n",
    "    \n",
    "    for dirpath, dirnames, files in os.walk(path):\n",
    "        for name in files:\n",
    "            all_files.append(os.path.join(dirpath, name))\n",
    "            \n",
    "            \n",
    "    for file_path in all_files:\n",
    "        if '/neg' in file_path and len(neg_data) <= _limit:\n",
    "            example = {}\n",
    "            with open(file_path, 'r') as myfile:\n",
    "                example['text'] = myfile.read().replace('\\n', '')\n",
    "            example['label'] = 0\n",
    "            neg_data.append(example)\n",
    "            \n",
    "        if '/pos' in file_path and len(pos_data) <= _limit:\n",
    "            example = {}\n",
    "            with open(file_path, 'r') as myfile:\n",
    "                example['text'] = myfile.read().replace('\\n', '')\n",
    "            example['label'] = 1\n",
    "            pos_data.append(example)\n",
    "    data = neg_data + pos_data\n",
    "\n",
    "    return data\n",
    "\n",
    "            \n",
    "imdb_test = load_imdb_data(imdb_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg = [\n",
    "    \"n't\",\n",
    "    \"not\",\n",
    "    \"none\",\n",
    "    \"no\",\n",
    "    \"never\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a function `feature_function()` that annotates datasets with feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_function(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "                         \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "\n",
    "    feature_names = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            #Extract features (by name) for one example:\n",
    "            word_counter = collections.Counter(tokenize(example['text']))\n",
    "            for x in word_counter.items():\n",
    "                if x[0] in vocabulary:\n",
    "                    example[\"features\"][\"word_count_for_\" + x[0]] = min(x[1], 1)\n",
    "                    \n",
    "            '''\n",
    "               Adding Negation feature\n",
    "            '''     \n",
    "            if any(n in example['text'].lower().split() for n in neg):\n",
    "                example[\"features\"][\"negation\"] = 1\n",
    "            elif \"n't\" in example['text']:\n",
    "                example[\"features\"][\"negation\"] = 1\n",
    "            else:\n",
    "                example[\"features\"][\"negation\"] = 0\n",
    "            \n",
    "            feature_names.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n",
    "    indices_to_features = {v: k for k, v in feature_indices.items()}\n",
    "    dim = len(feature_indices)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['vector'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['vector'][feature_indices[feature]] = example['features'][feature]\n",
    "    return indices_to_features\n",
    "    \n",
    "indices_to_features = feature_function([training_set, dev_set, imdb_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A linear classifier: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the classifier for this dataset. We’ll be using the LogisticRegression class from Scikit-learn.\n",
    "\n",
    "Install Scikit Learn from terminal:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our model to our dataset, we use Scikit-learn’s fit method to do so. This is where our ML classifier actually learns the underlying functions that produce our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [x['vector'] for x in training_set]\n",
    "y_train = [x['label'] for x in training_set]\n",
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we apply `log_model.predict()` to test the learned algorithm on the SST dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = [x['vector'] for x in dev_set]\n",
    "y_dev = [x['label'] for x in dev_set]\n",
    "\n",
    "y_dev_pred = log_model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at how accurate our function was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785550458716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_dev_pred, y_dev))\n",
    "# 0.779816513761"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "* Tweak the features in `feature_function()` to improve accuracy. For example, add a new feature to your model that captures the effect of negation (commented out in `feature_function()`, you can play around with this to add more conditions to capture negation from text). You can also try adding more features, for e.g., that detect words indicating positive or negative emotions.\n",
    "\n",
    "    * Keep testing how your accuracy evolves as you tweak your model.\n",
    "    * Let's now compare our results in a show of hands. Who did better than 80%?\n",
    "    \n",
    "<br> \n",
    "\n",
    "* You might have done well after tweaking your function over your development set. But you run into the risk that your model overfit the data you trained it on, i.e. it is too specialized and can only do well on that particular data. You can check whether this is the case on a new dataset, taken from IMDb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768924302789\n"
     ]
    }
   ],
   "source": [
    "x_test = [x['vector'] for x in imdb_test]\n",
    "y_test = [x['label'] for x in imdb_test]\n",
    "\n",
    "y_test_pred = log_model.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your model do as well?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
